{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outline\n",
    "- overview of R packages\n",
    "- installation using conda and yaml file\n",
    "- intro to foreach\n",
    "- advanced foreach features\n",
    "- explanation of \"back ends\" difference between doMC, doSNOW, doParallel, doMPI\n",
    "- reference sheet of parallel R packages\n",
    "- doMC and slurm on cluster\n",
    "- running on multiple nodes: doMPI and slurm\n",
    "- scheduling (chunking)\n",
    "- avoiding excess data copying\n",
    "- a real example, with performance evaluation\n",
    " - kmeans with multiple starts (single foreach)\n",
    " - intro to nested foreach\n",
    " - kmeans with loop for k and multiple starts\n",
    " \n",
    "- Rmpi\n",
    "- n body?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel R Packages\n",
    "\n",
    "- foreach: high level, parallel for loop method\n",
    "- snow: original multihost, tcp/ssh method \n",
    "- multicore: original forking method\n",
    "- parallel: unification of multicore and snow\n",
    "- Rmpi: interface to MPI for advanced parallel programming\n",
    "\n",
    "We will focus on __foreach__:\n",
    "- easy to use\n",
    "- most general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreach\n",
    "- designed and implemented by Steve Weston\n",
    "- natural semantics, similar to _for_ loop\n",
    "- [vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf)\n",
    "- similar to lapply\n",
    "- works on multiple cpus on one machine, or on many machines\n",
    "\n",
    "Foreach iterates over 1 or more indices, executes an expression on those indices, and returns a collection, by default a list:\n",
    "```\n",
    "foreach (arguments ...) %dopar% expr\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup for \"forked\" parallelism\n",
    "\n",
    "library(foreach)\n",
    "library(doMC)\n",
    "\n",
    "registerDoMC(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res <- foreach(i=1:10) %dopar% {\n",
    "    i*i\n",
    "}\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# simple function that simulates computing for set time\n",
    "spin<-function(sec) {\n",
    "\n",
    "  start<-proc.time()[[3]]\n",
    "\n",
    "  while (TRUE) {\n",
    "    z=1;\n",
    "    for (i in 1:100000) {\n",
    "      z<-z+1\n",
    "    }\n",
    "    now<-proc.time()[[3]]\n",
    "    if (now-start > sec) { break }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "f<-function(i){\n",
    "    spin(i)\n",
    "    i*i\n",
    "}\n",
    "\n",
    "system.time({\n",
    "res<-foreach (i=1:8) %dopar% \n",
    "{\n",
    "   f(i)\n",
    "}\n",
    "})\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rather than use a function, we can just put code in the block directly\n",
    "res<-foreach (i=1:8) %dopar% \n",
    "{\n",
    "    spin(i)\n",
    "    i*i\n",
    "}\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combiners\n",
    "Rather than returning the raw list, we can combine the values.  Foreach supports many combiners, using the named parameter .combine:\n",
    " - \"c\", \"+\", \"*\", \"cbind\", \"rbind\"\n",
    " - arbitrary user-supplied function of two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here, we add all of the results into a single integer\n",
    "res<-foreach (i=1:8, .combine=\"+\") %dopar% \n",
    "{\n",
    "    spin(i)\n",
    "    i*i\n",
    "}\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1:8 * 1:8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mymax <- function(a,b) {\n",
    "    if (a>b) a else b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User supplied combiner function.  \n",
    "res<-foreach (i=1:8, .combine=mymax) %dopar% \n",
    "{\n",
    "    spin(i)\n",
    "    i*i\n",
    "}\n",
    "\n",
    "res\n",
    "\n",
    "res<-foreach (i=1:8, .combine=\"mymax\") %dopar% \n",
    "{\n",
    "    spin(i)\n",
    "    i*i\n",
    "}\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Be very careful not to forget the dot!\n",
    "# Here, combine is an iteration variable with only 1 value\n",
    "# So, we only use 1 value from i, and iterate once!\n",
    "\n",
    "res<-foreach (i=1:10, combine=\"+\") %dopar% \n",
    "{\n",
    "    spin(i)\n",
    "    i*i\n",
    "}\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doMC Foreach batch script\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -c 4 \n",
    "\n",
    "module load miniconda\n",
    "source activate parallel_r\n",
    "R --slave -f ex1.R\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using multiple nodes\n",
    "- Thus far, we've only used multiple cpus on a single node, via doMC\n",
    "- Advantages (simple to use, no need to communicate)\n",
    "- Limits the degree of parallelism (e.g. 28)\n",
    "- Using the MPI \"backend\" allows us to scale to 100s or 1000s of cpus\n",
    "- No change required to code body. We just create and register a different backend \n",
    "- Make sure to clean up at end:\n",
    "```\n",
    "closeCluster(cl)\n",
    "mpi.quit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to R code to use MPI backend\n",
    "replace\n",
    "```\n",
    "library(doMC)\n",
    "registerDoMC(cores)\n",
    "```\n",
    "with\n",
    "```\n",
    "library(doMPI)\n",
    "startMPIcluster(verbose=TRUE, logdir=\"log\")\n",
    "registerDoMPI(cl)\n",
    "...\n",
    "\n",
    "closeCluster(cl)\n",
    "mpi.quit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doMPI Foreach batch script\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH -n 4\n",
    "\n",
    "module load miniconda\n",
    "source activate parallel_r\n",
    "mpirun R --slave -f ex1mpi.R\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on using multiple nodes\n",
    "- It is possible to use other backends (doParallel or doSnow) to run on multiple nodes.  We recommend doMPI+slurm\n",
    "- It is possible to specify the number of workers: ```startMPIcluster(count=3)``` but best to let slurm handle it\n",
    "- The number of workers is slurm ntasks-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans\n",
    "- classic method for clustering data.  You supply the number of clusters; the algorithm creates clusters that minimize total distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rides<-read.csv('Examples/Data/uber-raw-data-jun14.csv')\n",
    "locs<-rides[,c('Lat','Lon')]\n",
    "head(locs)\n",
    "dim(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res<-kmeans(locs,5,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res$centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(plyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrs<-as.data.frame(res$centers)\n",
    "#ctrs<-rename(ctrs, c(\"Lon\"=\"x\", \"Lat\"=\"y\"))\n",
    "# get_googlemap really wants the df with lon, lat\n",
    "ctrs<-ctrs[,c(2,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('ggmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(ggmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#register_google('AIzaSyDUCF__OmsszjklDTdqRWDTr6g7tL6Emqs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes\n",
    "to make the following work, I had to:\n",
    "- eable maps static api and geocoding api on the google console for my project, and get an api key (below)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_google('AIzaSyDA3cINLKAOjLWNiYuwXvOvfbZMb7FBHvs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "center<-geocode('grand central')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_map(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p<-ggmap(get_googlemap(center=c(lon=-73.97723, lat=40.75273)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p<-ggmap(get_googlemap(center=c(lon=-73.97723, lat=40.75273)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ggmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ggmap(get_googlemap(center=c(lon=-73.97723, lat=40.75273), markers = ctrs, zoom=10, scale = 1))\n",
    "#ggmap(get_googlemap(center=c(lon=-73.97723, lat=40.75273), markers = ctrs, path = ctrs, zoom=10, scale = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- round(data.frame(\n",
    "x = jitter(rep(-95.36, 50), amount = .3),\n",
    "y = jitter(rep( 29.76, 50), amount = .3)\n",
    "), digits = 2)\n",
    "map <- get_googlemap('houston', markers = df, scale = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cores<-strtoi(Sys.getenv('SLURM_CPUS_PER_TASK', unset=1))\n",
    "print(cores)\n",
    "registerDoMC(cores)\n",
    "\n",
    "starts=10\n",
    "tasks=cores\n",
    "nstarts=rep(starts/tasks, tasks)\n",
    "clusters=seq(2,10)\n",
    "\n",
    "print(c(\"tasks\", tasks, \"nstarts\", nstarts, \"clusters\", clusters))\n",
    "\n",
    "system.time({\n",
    "results <-\n",
    "  foreach (nstart=nstarts) %:%\n",
    "    foreach (cluster=clusters) %dopar% {\n",
    "  kmeans(locs, cluster, nstart=nstart)\n",
    "}})\n",
    "\n",
    "results <- unlist(results, recursive=FALSE)\n",
    "i = sapply(results, function(result) result$tot.withinss)\n",
    "result = results[[which.min(i)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_ctrs=as.data.frame(result$centers)[,c(2,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggmap(get_googlemap(center=c(lon=-73.97723, lat=40.75273), markers = par_ctrs, zoom=10, scale = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing your own Parallel R environment on linux or macos\n",
    "We recommend using conda\n",
    "\n",
    "```\n",
    "$ module load miniconda\n",
    "$ conda create --name parallel_r -c conda-forge r-base r-essentials r-doMC r-Rmpi\n",
    "```\n",
    "\n",
    "This step must be done on login node\n",
    "```\n",
    "$ R\n",
    "> install.packages('doMPI')\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
